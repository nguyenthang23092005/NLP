import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel
import os
from pathlib import Path
import tempfile

# Import libraries for file processing
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    from docx2txt import process as docx2txt_process
    DOC_AVAILABLE = True
except ImportError:
    DOC_AVAILABLE = False


# ============== Configuration ==============
st.set_page_config(
    page_title="Vietnamese Text Summarization",
    page_icon="üìù",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stTextArea textarea {
        font-size: 1.1rem;
    }
    .summary-box {
        background-color: #f0f8ff;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #1E88E5;
    }
    .stats-box {
        background-color: #f5f5f5;
        padding: 15px;
        border-radius: 8px;
        margin: 10px 0;
    }
    </style>
""", unsafe_allow_html=True)


# ============== File Processing Functions ==============
def extract_text_from_pdf(file):
    """Extract text from PDF file"""
    if not PDF_AVAILABLE:
        st.error("PyPDF2 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install PyPDF2")
        return None
    
    try:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc PDF: {str(e)}")
        return None


def extract_text_from_docx(file):
    """Extract text from DOCX file"""
    if not DOCX_AVAILABLE:
        st.error("python-docx ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install python-docx")
        return None
    
    try:
        doc = docx.Document(file)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc DOCX: {str(e)}")
        return None


def extract_text_from_doc(file):
    """Extract text from DOC file"""
    if not DOC_AVAILABLE:
        st.error("docx2txt ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install docx2txt")
        return None
    
    try:
        # Save to temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.doc') as tmp_file:
            tmp_file.write(file.getvalue())
            tmp_path = tmp_file.name
        
        # Extract text
        text = docx2txt_process(tmp_path)
        
        # Clean up
        os.unlink(tmp_path)
        
        return text.strip() if text else None
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc DOC: {str(e)}")
        return None


# ============== Model Loading ==============
@st.cache_resource
def load_model(model_path, base_model="google/mt5-small"):
    """Load model with caching"""
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        with st.spinner(f"ƒêang t·∫£i model t·ª´ {model_path}..."):
            tokenizer = AutoTokenizer.from_pretrained(base_model)
            
            # Check if it's a LoRA model
            if os.path.exists(os.path.join(model_path, "adapter_config.json")):
                # Load LoRA model
                base = AutoModelForSeq2SeqLM.from_pretrained(base_model)
                model = PeftModel.from_pretrained(base, model_path)
                model = model.merge_and_unload()
            else:
                # Load full fine-tuned model
                model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
            
            model.to(device)
            model.eval()
            
            return model, tokenizer, device
    
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i model: {str(e)}")
        return None, None, None


# ============== Summarization Function ==============
def generate_summary(
    text,
    model,
    tokenizer,
    device,
    max_source_length=4096,
    max_target_length=512,
    num_beams=5,
    temperature=0.8,
    top_p=0.92,
    do_sample=True,
    repetition_penalty=1.2
):
    """Generate summary for input text"""
    
    if not text or not text.strip():
        return None
    
    # CRITICAL FIX: D√πng prefix ƒë√∫ng cho mT5
    input_text = f"summarize: {text}"
    
    inputs = tokenizer(
        input_text,
        max_length=max_source_length,
        truncation=True,
        return_tensors="pt",
        padding=False
    )
    
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_target_length,
            min_length=60,  
            num_beams=num_beams,
            temperature=temperature if do_sample else 1.0,
            top_p=top_p if do_sample else 1.0,
            do_sample=do_sample,
            early_stopping=True,
            repetition_penalty=repetition_penalty,
            no_repeat_ngram_size=3,
            length_penalty=1.2,  # TƒÉng t·ª´ 1.0 -> 1.2 ƒë·ªÉ ∆∞u ti√™n output d√†i h∆°n
            # CRITICAL: Force decoder to start properly (not with pad)
            decoder_start_token_id=tokenizer.pad_token_id,  # mT5 uses pad as decoder start
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # Decode v√† l√†m s·∫°ch output
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # DEBUG: Ph√¢n t√≠ch chi ti·∫øt tokens
    print(f"\n{'='*60}")
    print(f"DEBUG - Token Analysis:")
    print(f"{'='*60}")
    print(f"Total tokens: {len(outputs[0])}")
    print(f"Output tokens: {outputs[0].tolist()}")
    
    # Decode t·ª´ng token ƒë·ªÉ xem
    print(f"\nToken breakdown:")
    for i, token_id in enumerate(outputs[0].tolist()):
        token_text = tokenizer.decode([token_id])
        token_name = tokenizer.convert_ids_to_tokens([token_id])[0]
        print(f"  [{i}] ID={token_id:6d} | Token='{token_name:15s}' | Text='{token_text}'")
    
    # Decode v·ªõi skip_special_tokens
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nDecoded (skip_special_tokens=True): '{summary}'")
    
    # Decode kh√¥ng skip ƒë·ªÉ so s√°nh
    summary_with_special = tokenizer.decode(outputs[0], skip_special_tokens=False)
    print(f"Decoded (skip_special_tokens=False): '{summary_with_special}'")
    print(f"{'='*60}\n")
    
    # Post-processing: Lo·∫°i b·ªè c√°c token l·∫° c√≤n s√≥t l·∫°i
    summary = summary.replace("<extra_id_0>", "").replace("<extra_id_1>", "")
    summary = summary.replace("<extra_id_2>", "").replace("<extra_id_3>", "")
    summary = summary.replace("<pad>", "").replace("</s>", "").replace("<s>", "")
    summary = summary.strip()
    
    # Ki·ªÉm tra n·∫øu summary kh√¥ng h·ª£p l·ªá
    if not summary or len(summary) < 15:
        print(f"‚ö†Ô∏è WARNING: Summary too short or invalid: '{summary}'")
        return "‚ö†Ô∏è Model ch∆∞a ƒë∆∞·ª£c train ƒë√∫ng. Vui l√≤ng train l·∫°i model ho·∫∑c ch·ªçn model kh√°c."
    
    # Ki·ªÉm tra n·∫øu summary ch·ªâ l√† copy t·ª´ input (d·∫•u hi·ªáu model ch∆∞a h·ªçc)
    if summary in text:
        print(f"‚ö†Ô∏è WARNING: Summary is just a substring from input")
        return f"‚ö†Ô∏è Model ƒëang copy vƒÉn b·∫£n g·ªëc thay v√¨ t√≥m t·∫Øt. K·∫øt qu·∫£: '{summary}'"
    
    return summary


# ============== Main App ==============
def main():
    # Header
    st.markdown('<div class="main-header">üìù T√≥m T·∫Øt VƒÉn B·∫£n Ti·∫øng Vi·ªát</div>', unsafe_allow_html=True)
    
    # Sidebar - Model Selection
    st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")
    
    # Model selection
    model_options = {
        "LoRA + CPO": "./models/mt5-cpo-full/checkpoint-1500",
        "LoRA v1": "./models/mt5-lora-full/checkpoint-7728",
        "LoRA v2": "./models/mt5-lora-v2/checkpoint-5000",
        "Base mT5": "google/mt5-small"
    }
    
    # Find available models
    available_models = {}
    for name, path in model_options.items():
        if name == "Base mT5" or os.path.exists(path):
            available_models[name] = path
    
    if not available_models:
        st.error("Kh√¥ng t√¨m th·∫•y model n√†o! Vui l√≤ng train model tr∆∞·ªõc.")
        return
    
    # Default to LoRA SFT model if available
    default_index = 0
    if "LoRA SFT (Khuy√™n d√πng)" in available_models:
        default_index = list(available_models.keys()).index("LoRA SFT (Khuy√™n d√πng)")
    
    selected_model_name = st.sidebar.selectbox(
        "Ch·ªçn model:",
        options=list(available_models.keys()),
        index=default_index,
        help="Ch·ªçn model ƒë·ªÉ s·ª≠ d·ª•ng cho t√≥m t·∫Øt"
    )
    
    model_path = available_models[selected_model_name]
    
    # Fixed parameters (kh√¥ng hi·ªÉn th·ªã tr√™n UI) - TƒÉng ƒë·ªô d√†i cho t√≥m t·∫Øt phong ph√∫ h∆°n
    max_source_length = 2048  # TƒÉng t·ª´ 512 -> 1024 ƒë·ªÉ nh·∫≠n vƒÉn b·∫£n d√†i h∆°n
    max_target_length = 512   # TƒÉng t·ª´ 128 -> 256 ƒë·ªÉ t√≥m t·∫Øt chi ti·∫øt h∆°n
    num_beams = 5             # TƒÉng t·ª´ 4 -> 5 ƒë·ªÉ t√¨m ki·∫øm t·ªët h∆°n
    repetition_penalty = 1.3  # TƒÉng t·ª´ 1.2 -> 1.3 ƒë·ªÉ gi·∫£m l·∫∑p t·ª´
    do_sample = True          # B·∫≠t sampling ƒë·ªÉ ƒëa d·∫°ng h∆°n
    temperature = 0.8         # Nhi·ªát ƒë·ªô 0.8 c√¢n b·∫±ng gi·ªØa ƒëa d·∫°ng v√† ch·∫•t l∆∞·ª£ng
    top_p = 0.92              # Top-p cao h∆°n cho nhi·ªÅu l·ª±a ch·ªçn t·ª´
    
    # Device info
    device_type = "üéÆ GPU" if torch.cuda.is_available() else "üíª CPU"
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        st.sidebar.info(f"{device_type}: {gpu_name}")
    else:
        st.sidebar.warning(f"{device_type} (ch·∫≠m h∆°n)")
    
    # Load model
    model, tokenizer, device = load_model(model_path)
    
    if model is None:
        st.error("Kh√¥ng th·ªÉ t·∫£i model!")
        return
    
    st.sidebar.success(f"‚úÖ Model ƒë√£ s·∫µn s√†ng: {selected_model_name}")
    
    # Main content
    st.markdown("---")
    
    # Input method selection
    input_method = st.radio(
        "Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p:",
        options=["üìù Nh·∫≠p vƒÉn b·∫£n tr·ª±c ti·∫øp", "üìÑ Upload file (PDF, DOC, DOCX)"],
        horizontal=True
    )
    
    input_text = None
    
    if input_method == "üìù Nh·∫≠p vƒÉn b·∫£n tr·ª±c ti·∫øp":
        # Text input
        input_text = st.text_area(
            "Nh·∫≠p vƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt:",
            height=250,
            placeholder="Nh·∫≠p ho·∫∑c d√°n vƒÉn b·∫£n ti·∫øng Vi·ªát v√†o ƒë√¢y...",
            help="Nh·∫≠p vƒÉn b·∫£n b·∫°n mu·ªën t√≥m t·∫Øt",
            key="input_text_area"
        )
        
        # Ki·ªÉm tra n·∫øu input thay ƒë·ªïi th√¨ x√≥a k·∫øt qu·∫£ c≈©
        if 'last_input' not in st.session_state:
            st.session_state.last_input = ""
        
        if input_text != st.session_state.last_input:
            st.session_state.last_input = input_text
            if 'summary_result' in st.session_state:
                st.session_state.summary_result = None
        
        # Output area - hi·ªÉn th·ªã k·∫øt qu·∫£ t√≥m t·∫Øt
        if 'summary_result' in st.session_state and st.session_state.summary_result:
            st.markdown("#### üìù K·∫øt qu·∫£ t√≥m t·∫Øt:")
            st.text_area(
                "T√≥m t·∫Øt:",
                value=st.session_state.summary_result,
                height=150,
                disabled=False,
                key="output_text_area",
                help="B·∫°n c√≥ th·ªÉ ch·ªânh s·ª≠a k·∫øt qu·∫£ t√≥m t·∫Øt t·∫°i ƒë√¢y"
            )
    
    else:
        # File upload
        uploaded_file = st.file_uploader(
            "Upload file:",
            type=['pdf', 'doc', 'docx'],
            help="H·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng: PDF, DOC, DOCX"
        )
        
        if uploaded_file is not None:
            file_type = uploaded_file.name.split('.')[-1].lower()
            
            with st.spinner(f"ƒêang ƒë·ªçc file {uploaded_file.name}..."):
                if file_type == 'pdf':
                    input_text = extract_text_from_pdf(uploaded_file)
                elif file_type == 'docx':
                    input_text = extract_text_from_docx(uploaded_file)
                elif file_type == 'doc':
                    input_text = extract_text_from_doc(uploaded_file)
                else:
                    st.error(f"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_type}")
            
            if input_text:
                st.success(f"‚úÖ ƒê√£ ƒë·ªçc th√†nh c√¥ng file: {uploaded_file.name}")
                
                # Show extracted text in expander
                with st.expander("üìÑ Xem n·ªôi dung ƒë√£ tr√≠ch xu·∫•t"):
                    st.text_area("VƒÉn b·∫£n t·ª´ file:", input_text, height=200, disabled=True)
        
        # Output area - hi·ªÉn th·ªã k·∫øt qu·∫£ t√≥m t·∫Øt cho file upload
        if 'summary_result' in st.session_state and st.session_state.summary_result:
            st.markdown("#### üìù K·∫øt qu·∫£ t√≥m t·∫Øt:")
            st.text_area(
                "T√≥m t·∫Øt:",
                value=st.session_state.summary_result,
                height=150,
                disabled=False,
                key="output_text_area_file",
                help="B·∫°n c√≥ th·ªÉ ch·ªânh s·ª≠a k·∫øt qu·∫£ t√≥m t·∫Øt t·∫°i ƒë√¢y"
            )
    
    # Summarize button
    st.markdown("---")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        summarize_button = st.button(
            "üöÄ T√ìM T·∫ÆT NGAY",
            type="primary",
            use_container_width=True
        )
    
    # Generate summary
    if summarize_button:
        if not input_text or not input_text.strip():
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ho·∫∑c upload file!")
        else:
            # X√≥a k·∫øt qu·∫£ c≈© tr∆∞·ªõc khi t√≥m t·∫Øt m·ªõi
            if 'summary_result' in st.session_state:
                st.session_state.summary_result = None
            
            # Show input statistics
            input_words = len(input_text.split())
            input_chars = len(input_text)
            
            st.markdown("### üìä Th√¥ng tin vƒÉn b·∫£n ƒë·∫ßu v√†o")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("S·ªë t·ª´", f"{input_words:,}")
            with col2:
                st.metric("S·ªë k√Ω t·ª±", f"{input_chars:,}")
            with col3:
                estimated_time = max(1, input_words // 100)
                st.metric("Th·ªùi gian ∆∞·ªõc t√≠nh", f"~{estimated_time}s")
            
            # Generate summary
            with st.spinner("‚è≥ ƒêang t√≥m t·∫Øt vƒÉn b·∫£n..."):
                summary = generate_summary(
                    text=input_text,
                    model=model,
                    tokenizer=tokenizer,
                    device=device,
                    max_source_length=max_source_length,
                    max_target_length=max_target_length,
                    num_beams=num_beams,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    repetition_penalty=repetition_penalty
                )
                
                # L∆∞u k·∫øt qu·∫£ v√†o session state ƒë·ªÉ hi·ªÉn th·ªã ·ªü text area
                if summary and len(summary.strip()) > 0:
                    st.session_state.summary_result = summary
                    # C·∫≠p nh·∫≠t last_input ƒë·ªÉ tr√°nh x√≥a k·∫øt qu·∫£ m·ªõi
                    st.session_state.last_input = input_text
            
            # Rerun ƒë·ªÉ c·∫≠p nh·∫≠t UI v·ªõi k·∫øt qu·∫£ trong text area
            if summary and len(summary.strip()) > 0:
                st.rerun()
            
            # Show statistics and download options below the text areas
            if 'summary_result' in st.session_state and st.session_state.summary_result:
                summary = st.session_state.summary_result
                
                # Summary statistics
                summary_words = len(summary.split())
                summary_chars = len(summary)
                compression_ratio = (1 - summary_words / input_words) * 100 if input_words > 0 else 0
                
                st.markdown("### üìà Th·ªëng k√™ t√≥m t·∫Øt")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("S·ªë t·ª´", f"{summary_words}")
                with col2:
                    st.metric("S·ªë k√Ω t·ª±", f"{summary_chars}")
                with col3:
                    st.metric("T·ª∑ l·ªá n√©n", f"{compression_ratio:.1f}%")
                with col4:
                    st.metric("Model", selected_model_name.split('(')[0].strip())
                
                # Download options
                st.markdown("---")
                st.markdown("### üíæ T·∫£i xu·ªëng k·∫øt qu·∫£")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # Download as text
                    result_text = f"VƒÇN B·∫¢N G·ªêC:\n{input_text}\n\n{'='*50}\n\nT√ìM T·∫ÆT:\n{summary}\n\n{'='*50}\n\nTH·ªêNG K√ä:\n- VƒÉn b·∫£n g·ªëc: {input_words} t·ª´, {input_chars} k√Ω t·ª±\n- T√≥m t·∫Øt: {summary_words} t·ª´, {summary_chars} k√Ω t·ª±\n- T·ª∑ l·ªá n√©n: {compression_ratio:.1f}%\n- Model: {selected_model_name}"
                    
                    st.download_button(
                        label="üìÑ T·∫£i xu·ªëng (.txt)",
                        data=result_text,
                        file_name="tom_tat.txt",
                        mime="text/plain"
                    )
                
                with col2:
                    # Copy to clipboard button (visual only)
                    st.button("üìã Sao ch√©p t√≥m t·∫Øt", help="Sao ch√©p t√≥m t·∫Øt v√†o clipboard")
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt. Vui l√≤ng th·ª≠ l·∫°i!")


if __name__ == "__main__":
    main()
